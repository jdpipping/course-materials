% Page setup
\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0 in}
\setlength{\evensidemargin}{0 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

% Essential packages
\usepackage{amsmath,amsfonts,amsthm,graphicx,bm,bbm}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{float}
\usepackage{footmisc}
\usepackage{color}
\renewcommand{\thefootnote}{\alph{footnote}}

% Counter setup for lecture numbering
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

% Lecture header command
\newcommand{\lecture}[6]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf #3 \hfill #4} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Instructor: #5 \hfill Scribe: #6} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
}

% Citation and reference commands
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

% Figure command
\newcommand{\fig}[3]{
\vspace{#2}
\begin{center}
Figure \thelecnum.#1:~#3
\end{center}
}

% Theorem environments
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}{\normalfont}

% Common math symbols
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}

\begin{document}

\lecture{10}{Priors and the Power of Fake Data}{WSABI Summer Research Lab}{Summer 2024}{Ryan Brill}{Jonathan Pipping}

\section{Motivating Example: End of Season Win Percentage}

\subsection{Problem Setup}

Suppose an MLB team $T$ has won $W$ games and lost $L$ games so far this season. How would you predict their end-of-season win percentage $WP$? There are some constraints on this problem:
\begin{itemize}
    \item[-] We have no access to team $T$'s past or future schedule.
    \item[-] We have no access to team $T$'s previous season's data.
\end{itemize}
Together, these constraints rule out implementing regression models or controlling for strength of schedule. With this in mind, how can we guess their end of season win percentage? If you asked the average person, they'd probably guess their current win percentage:
\begin{equation}
    \widehat{WP} = \frac{W}{W + L}
\end{equation}
This is a reasonable guess, but there are some problems with it. For example, what if team $T$ has only played a few games? If $W = 3$ and $L = 0$, then $\widehat{WP} = 1$. We clearly don't expect team $T$ to win all 162 games they play, so we need to look for a better estimate.

\subsection{Idea: Adding Fake Data}

Suppose now that team $T$ begins the season with $W'$ fake wins and $L'$ fake losses. Then a new guess for their end-of-season win percentage is:
\begin{equation}
    \widehat{WP}' = \frac{W' + W}{(W' + W) + (L' + L)}
\end{equation}
For concreteness, let's consider the example from earlier: $W = 3$ and $L = 0$. If we use Tom Tango's method of setting $W' = L' = 15$, then our adjusted estimate is:
\begin{equation}
    \widehat{WP}' = \frac{15 + 3}{(15 + 3) + (15 + 0)} = \frac{18}{33} \approx 0.546
\end{equation}
This is a very different estimate than $\widehat{WP} = 1$, but it's a much more stable one. What about later in the season? If team $T$ has won $W = 45$ and lost $L = 30$ games, then our two estimates are:
\begin{align*}
    \widehat{WP} &= \frac{45}{45 + 30} = 0.6 \\
    \widehat{WP}' &= \frac{15 + 45}{(15 + 45) + (15 + 30)} = \frac{60}{105} \approx 0.571
\end{align*}
As the season progresses, the influence of the fake data is less and less, and $\widehat{WP}'$ and $\widehat{WP}$ get closer together. Still, which of these two estimates is better? We will have to formalize our estimates to answer this question.

\subsection{Formalizing the Problem}

Team $T$ plays $n = 162$ games in a season. Suppose, for simplicity, that team $T$ wins each game with probability $p$. Then $\{X_1, \hdots, X_n\}$ represent game outcomes, where
\begin{equation}
    X_i = \begin{cases}
        1 & \text{w.p. } p \\
        0 & \text{w.p. } 1 - p
    \end{cases} \stackrel{d}{=} \text{Bernoulli}(p)
\end{equation}
Suppose we have observed $m$ games thus far in the season. Then our observed data is $\{X_1, \hdots, X_m\}$. Then our observed number of wins is
\begin{equation}
    W = \sum_{i=1}^m X_i \sim \text{Binomial}(m, p)
\end{equation}
since it is the sum of $m$ i.i.d. Bernoulli$(p)$ random variables. We then express the end-of-sesason win percentage as
\begin{equation}
    WP \sim \frac{1}{n} \text{Binomial}(n, p)
\end{equation}
Then we plan to use the observed data to estimate $p$ (call it $\widehat{p}$), and then use $\widehat{p}$ to estimate $WP$ as follows:
\begin{equation}
    \widehat{WP} = \frac{1}{n} \mathbb{E}[WP] = \frac{1}{n} \mathbb{E}\left[\text{Binomial}(n, p)\right] = \frac{1}{n} \cdot n\widehat{p} = \widehat{p}
\end{equation}
To accomplish this, we will explore the concept of the Maximum Likelihood Estimator (MLE).

\section{Maximum Likelihood Estimator}

\subsection{Definition}

The method of maximum likelihood estimation involves selecting the parameter estimate $\widehat{\theta}$ which maximizes the probability of observing the data we observed under the proposed model. To formalize this notion, we will first need to define the Likelihood function.

\begin{definition}[Likelihood]\label{def:likelihood}
    If $\{X_i\}_{i=1}^n$ represents observed data drawn from a distribution $\mathbb{P}$ parametrized by parameter vector $\boldsymbol{\theta}$, then the Likelihood of observed data $\{X_i\}_{i=1}^n$ is defined as
    \begin{equation}
        \mathcal{L}(X_1, \hdots, X_n \mid \boldsymbol{\theta}) = \mathbb{P}(X_1, \hdots, X_n \mid \boldsymbol{\theta})
    \end{equation}
\end{definition}
In other words, the Likelihood represents the joint probability of observing the data we did conditional on the parameter(s) $\boldsymbol{\theta}$ of the underlying distribution. We are now ready to define the Maximum Likelihood Estimator (or MLE).

\begin{definition}[Maximum Likelihood Estimator]\label{def:mle}
    If $\Theta$ represents the parameter space, $\{X_i\}_{i = 1}^n$ represents the observed data, and $\mathcal{L}(X_1, \hdots, X_n \mid \boldsymbol{\theta)}$ represents the likelihood of data $\{X_i\}_{i=1}^n$, the Maximum Likelihood Estimator is the solution to the equation:
    \begin{equation}
        \widehat{\theta}_{MLE} = \underset{\boldsymbol{\theta} \in \Theta}{\arg\max} \mathcal{L}(X_1, \hdots, X_n \mid \boldsymbol{\theta})
    \end{equation}
\end{definition}
We'll now apply this method to our problem of predicting team $T$'s end-of-season winning percentage.

\subsection{Applying to Our Problem}

Recall that $\{X_i\}_{i=1}^m$ are i.i.d. Bernoulli$(p)$ random variables representing the observed games played by team $T$. Then by Definition \ref{def:mle}, the Maximum Likelihood Estimator for $p$ is given by:
\begin{align}
    \widehat{p}_{MLE}&= \underset{p}{\arg\max} \mathcal{L}(X_1, \hdots, X_m \mid p) \\
    &= \underset{p}{\arg\max}\mathbb{P}(X_1, \hdots, X_m \mid p) \text{ from Definition \ref{def:likelihood}} \nonumber \\
    &= \underset{p}{\arg\max}\prod_{i=1}^m \mathbb{P}(X_i \mid p) \text{ by independence of } \{X_i\}_{i=1}^m \nonumber
\end{align}
Recall that the probability mass function of a Bernoulli$(p)$ random variable is given by $f(x) = p^x (1-p)^{(1-x)}$ for $x \in \{0, 1\}$. Then we have that
\begin{align}
    \widehat{p}_{MLE} &= \underset{p}{\arg\max}\prod_{i=1}^m p^{X_i}(1-p)^{1 - X_i} \text{ since } X_i \sim \text{ Bernoulli}(p) \\
    &= \underset{p}{\arg\max} p^{\sum_{i=1}^m X_i}(1-p)^{\sum_{i=1}^m (1-X_i)} \nonumber\
    \intertext{Since we know that $\sum_{i=1}^m X_i = W$ and $\sum_{i=1}^m (1-X_i) = L$, we have that}
    \widehat{p}_{MLE} &= \underset{p}{\arg\max} p^W(1-p)^L \\
    &= \underset{p}{\arg\max} \log\left[p^W\left(1-p\right)^L\right] \text{ since } \log \text{ is monotonic} \nonumber \\
    &= \underset{p}{\arg\max} W\log(p) + L\log(1-p) \nonumber
\end{align}
To maximize this expression, we take the derivative with respect to $p$, set it equal to 0, and solve for $p$.
\begin{align}
    \frac{\partial}{\partial p} \left(W\log\left(p\right) + L\log\left(1-p\right)\right) &= \frac{W}{p} - \frac{L}{1-p} = 0 \nonumber \\
    \implies \frac{W}{p} &= \frac{L}{1-p} \nonumber \\
    \implies \widehat{p}_{MLE} &= \frac{W}{W + L} 
\end{align}
But wait, this is the same formula from earlier! The MLE is simply the observed win percentage through $m$ games. Still,we know this is a bad estimate early in the season. Why did the MLE go wrong? How can we modify the MLE to get a better estimate?

Before, to improve our estimate of $WP$, we added some fake data $(W', L')$ to the observed data $(W, L)$, which allowed us to improve our estimate of $WP$ to
\begin{equation}
    \widehat{WP}' = \frac{W' + W}{(W' + W) + (L' + L)}
\end{equation}
In adding this fake data, we used \color{blue} prior information: \color{black} prior to the season, we assumed team $T$ had $W'$ wins and $L'$ losses. We introduce a way to formalize this notion of prior information in the next section.

\section{Introduction to Bayesian Statistics}

\subsection{Priors}

To this point in the course, we have taken a \textbf{frequentist} approach to parameter estimation, which assumes a model where the data is random while the parameter is fixed. An alternative approach is the \textbf{Bayesian} approach, which assumes models in which we treat parameters as random variables with their own probability distributions.

At first, the difference between the two approaches may not be obvious, but the Bayesian approach introduces added flexibility in the model through the introduction of \textbf{priors}. What is a prior?
\begin{definition}[Prior]
    A prior is a probability distribution over the parameter space $\Theta$ that represents our beliefs about the parameter before we observe any data.
\end{definition}
In our example, the addition of prior "fake" data essentially assigns a probability distribution to the parameter $p$ which reflects our \textbf{prior} belief on what $p$ is more likely to be! This allows for much more stable estimates of $WP$ early in the season where the MLE is very unstable. Let's continue formalizing our win probability model using the \textbf{Beta-Binomial model}.

\subsection{Beta-Binomial Model}

Recall that in our original model with observed data $\{X_i\}_{i=1}^m$, we had that
\begin{equation}
    W = \sum_{i=1}^m X_i \sim \text{Binomial}(m, p)
\end{equation}
and that
\begin{equation}
    \widehat{WP} = \frac{1}{n} \mathbb{E}[W] = \frac{1}{n} \text{Binomial}(n, p)
\end{equation}
We will not change this portion of the model, but we \textbf{will} add a prior distribution to the parameter $p$, specifically a Beta distribution.
\begin{definition}[Beta Distribution]
    The Beta distribution is characterized by two parameters, $\alpha$ and $\beta$, and has probability density function
    \begin{equation}
        f(x \mid \alpha, \beta) = C \cdot x^{\alpha - 1} (1-x)^{\beta - 1} \text{ for } x \in [0, 1], \alpha, \beta > 0
    \end{equation}
    where $C$ is a normalizating constant.
\end{definition}
We visualize the Beta distribution for different values of $\alpha$ and $\beta$ in Figure \ref{fig:beta-dist}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/10_beta-dist.png}
    \caption{PDF of the Beta distribution for different values of $\alpha$ and $\beta$.}
    \label{fig:beta-dist}
\end{figure}
Note that the Beta distribution is very flexible in shape, but is always constrained between 0 and 1: this makes it a natural choice as a prior for our probability parameter $p$.

We define our Beta-Binomial model as follows:
\begin{equation}
    \begin{cases}
        W \sim \text{Binomial}(m, p) \\
        p \sim \text{Beta}(\alpha, \beta)
    \end{cases}
    \text{where } \alpha = W' + 1 \text{ and } \beta = L' + 1
\end{equation}
Now, as before, we wish to estimate $p$. A Bayesian approach to parameter estimation should incorporate both the prior information and the observed data, and the maximum a-posteriori (MAP) estimate is a natural way to do this.

\subsection{Maximum a-Posteriori Estimate}

\begin{definition}[Maximum a-Posteriori (MAP) Estimate]\label{def:map}
    If $\Theta$ represents the parameter space, $\{X_i\}_{i = 1}^n$ represents the observed data, and $\mathbb{P}(\theta \mid X_1, \hdots, X_n)$ represents the posterior probability of parameter $\theta$ given the data, then the Maximum a-Posteriori (MAP) estimate is the solution to the equation:
    \begin{equation}
        \widehat{\theta}_{MAP} = \underset{\theta \in \Theta}{\arg\max} \mathbb{P}(\theta \mid X_1, \hdots, X_n)
    \end{equation}
    In other words, the MAP estimate is the parameter value that maximizes the posterior probability of the parameter given the observed data.
\end{definition}
Now let's apply this to our Beta-Binomial model to estimate $p$. Recalling that $W = \sum_{i=1}^m X_i$ encodes the observed data, we have that
\begin{align}
    \widehat{p}_{MAP} &= \underset{p}{\arg\max} \mathbb{P}(p \mid W) \text{ by Definition \ref{def:map}} \\
    &= \underset{p}{\arg\max} \frac{\mathbb{P}(W \mid p) \mathbb{P}(p)}{\mathbb{P}(W)} \text{ by Bayes' Rule} \nonumber \\
    &= \underset{p}{\arg\max} \color{blue}\mathbb{P}(W \mid p)\color{black}\color{red}\mathbb{P}(p)\color{black} \text{ since } \mathbb{P}(W) \text{ is constant w.r.t. } p \label{eq:map-estimate}
\end{align}
Note that in this expression, \color{blue}$\mathbb{P}(W \mid p)$ is the likelihood of the data given the parameter $p$, \color{black} and \color{red} $\mathbb{P}(p)$ is the prior distribution of the parameter $p$. \color{black} So then
\begin{align}
    \widehat{p}_{MAP} &= \underset{p}{\arg\max} \color{blue}\mathbb{P}(\text{Binomial}(m, p) = W)\color{black} \color{red}\mathbb{P}(\text{Beta}(\alpha, \beta) = p)\color{black} \nonumber \\
    &= \underset{p}{\arg\max} \color{blue}\binom{m}{W} p^W(1-p)^{m-W}\color{black} \cdot \color{red}C \cdot p^{\alpha - 1}(1-p)^{\beta - 1}\color{black} \nonumber
    \intertext{Removing constants with respect to $p$ and noting that $m = W + L$, we have that}
    &= \underset{p}{\arg\max} \color{blue}p^W(1-p)^{L}\color{black} \cdot \color{red}p^{\alpha - 1}(1-p)^{\beta - 1}\color{black} \nonumber \\
    \intertext{Then combining the exponents, we have that}
    &= \underset{p}{\arg\max} p^{W + \alpha - 1}(1-p)^{L + \beta - 1} \nonumber \\
    \intertext{From here, we follow the same procedure as before to maximize the expression: taking the log for simplicity, setting the derivative equal to 0, and solving for $p$. Following this procedure, we get that}
    \widehat{p}_{MAP} &= \frac{(W + \alpha - 1)}{(W + \alpha - 1) + (L + \beta - 1)}
    \intertext{If we let $\alpha = W' + 1$ and $\beta = L' + 1$, then we have that}
    &= \frac{W' + W}{(W' + W) + (L' + L)}
\end{align}
So the MAP estimate is simply the win percentage if we add $\alpha - 1$ fake wins and $\beta - 1$ fake losses to the observed data! If we wanted to make informed choices of $\alpha$ and $\beta$, we might use data from previous seasons to tune these parameters.

\subsection{Comparison of MLE and MAP}

Note that if we were to set $\alpha = \beta = 1$, then the prior Beta distribution would be uniform over $[0, 1]$. In this case, the MAP estimate would be
\begin{equation}
    \widehat{p}_{MAP} = \frac{W + 1 - 1}{(W + 1 - 1) + (L + 1 - 1)} = \frac{W}{W + L} = \widehat{p}_{MLE}
\end{equation}
So the MAP estimate is the same as the MLE when the prior is uniform. This is equivalent to setting up our model in the following way:
\begin{equation*}
    \begin{cases}
        W \sim \text{Binomial}(m, p) \\
        p \sim \text{Beta}(1, 1) \stackrel{d}{=} \text{Uniform}(0, 1)
    \end{cases}
\end{equation*}
This is known as an \textbf{uninformative prior} which encodes no preference for any particular value of $p$. When we calculate the MAP estimate from Equation \ref{eq:map-estimate}, we see that
\begin{align}
    \widehat{p}_{MAP} &= \underset{p}{\arg\max} \color{blue}\mathbb{P}(p \mid W)\color{black}\color{red}\mathbb{P}(p)\color{black} \nonumber \\
    &= \underset{p}{\arg\max} \color{blue}\mathbb{P}(p \mid W)\color{black}\color{red}\mathbb{P}(\text{Uniform}(0, 1) = p)\color{black} \nonumber \\
    &= \underset{p}{\arg\max} \color{blue}\mathbb{P}(p \mid W)\color{black}\color{red}(1)\color{black} \nonumber \\
    &= \widehat{p}_{MLE} \nonumber \\
    &= \frac{W}{W + L} \nonumber
\end{align}

\subsection{Takeaways}

\begin{itemize}
    \item[-] While frequentist statistics treats parameters (ex: $p$) as fixed, Bayesian statistics treats them as random variables with their own probability distributions.
    \item[-] This allows us to blend observed data with prior information, outside information not seen in the data, to achieve more stable estimators and make better predictions.
\end{itemize}

\end{document}