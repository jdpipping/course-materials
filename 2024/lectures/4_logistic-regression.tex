% Page setup
\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0 in}
\setlength{\evensidemargin}{0 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

% Essential packages
\usepackage{amsmath,amsfonts,amsthm,graphicx,bm,bbm}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{float}
\usepackage{footmisc}
\usepackage{color}
\renewcommand{\thefootnote}{\alph{footnote}}

% Counter setup for lecture numbering
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

% Lecture header command
\newcommand{\lecture}[6]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf #3 \hfill #4} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Instructor: #5 \hfill Scribe: #6} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
}

% Citation and reference commands
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

% Figure command
\newcommand{\fig}[3]{
\vspace{#2}
\begin{center}
Figure \thelecnum.#1:~#3
\end{center}
}

% Theorem environments
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}{\normalfont}

% Common math symbols
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}

\begin{document}

\lecture{4}{Logistic Regression}{WSABI Summer Research Lab}{Summer 2024}{Ryan Brill}{Jonathan Pipping}

\section{Motivating Example: Sinking Putts}

\subsection{Problem Setup}

\textbf{Question:} Predict the \textbf{probability} that a putt is sunk as a function of the distance to the hole.

\textbf{Data:} 5,988 putts from Columbia including distance to the hole and whether the putt was sunk or not. Each row in the dataset is a single putt and includes the following variables:
\begin{itemize}
    \item[-] $i$: index of the $i^{th}$ putt in our dataset
    \item[-] $y_i$: 1 if the $i^{th}$ putt was sunk, 0 otherwise
    \item[-] $x_i$: distance to the hole of the $i^{th}$ putt
\end{itemize}

\textbf{Visualization:} The first thing we should do is plot the data, which we do in Figure \ref{fig:putt-scatter}. What do you notice?
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/4_putt-scatter.png}
    \caption{Scatter plot of the putt data}
    \label{fig:putt-scatter}
\end{figure}
Like we would expect, the observed proportion of putts sunk decreases as the distance to the hole increases. Let's try to use what we've learned about linear regression to model this relationship.

\subsection{Modeling}

\textbf{Model 1: Simple Linear Regression}

We know from Lecture 1 that the this model takes the form:
\begin{equation}
    y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \quad \mathbb{E}[\epsilon_i] = 0
\end{equation}
We already know how to estimate the coefficients for this model. Recall that the least squares solution to this model is given by:
\begin{align}
    \widehat{\beta}_0 &= \bar{Y} - \widehat{\beta}_1 \bar{X} \\
    \widehat{\beta}_1 &= \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2}
\end{align}
We fit this model in \texttt{R} with the following code:
\begin{lstlisting}[language=R]
    model_1 = lm(putt_made ~ distance, data = putt_data)
    summary(model_1)
\end{lstlisting}
When we plot the fitted line against the data in Figure \ref{fig:putt-simple}, we see that the model does not fit the data well.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/4_putt-simple.png}
    \caption{Putt data and fitted simple linear regression}
    \label{fig:putt-simple}
\end{figure}
We then move on to a more complex model: a cubic regression similar to the quadratic model we fit in Lecture 2.

\textbf{Model 2: Cubic Regression}

We know the form of this model as well:
\begin{equation}
    y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i, \quad \mathbb{E}[\epsilon_i] = 0
\end{equation}
We also know how to estimate these coefficients from Lecture 2. Recall that the least squares solution to this model is given by:
\begin{align}
    \widehat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
\end{align}
We fit this model in \texttt{R} with the following code:
\begin{lstlisting}[language=R]
    model_2 = lm(putt_made ~ distance + I(distance^2) + I(distance^3),
                 data = putt_data)
    summary(model_2)
\end{lstlisting}
When we plot the fitted line against the data in Figure \ref{fig:putt-cubic}, we see that the model fits the data much better when $x_i \in [0, 20]$.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/4_putt-cubic.png}
    \caption{Putt data and fitted cubic regression}
    \label{fig:putt-cubic}
\end{figure}
However, when we zoom out in Figure \ref{fig:putt-cubic-zoomed}, we see that the model is unable to extrapolate when $x_i > 20$: in fact, the estimated probability of sinking the putt goes below 0.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/4_putt-cubic-zoomed.png}
    \caption{Putt data and fitted cubic regression (zoomed out)}
    \label{fig:putt-cubic-zoomed}
\end{figure}
This presents a problem: we need our predictions $\widehat{y}_i$ to be between 0 and 1, but ordinary linear regression does \textbf{not} guarantee this. How can we constrain our model to ensure that our predictions in $[0, 1]$?

\section{Logistic Regression}

\subsection{The Logistic Function}

We are looking for a "squishification" function that takes numbers in $\mathbb{R}$ and maps them to numbers in $[0, 1]$. One such function is the \textbf{logistic  (or sigmoid) function}:
\begin{definition}[Logistic Function]
    The \textbf{logistic function} is defined as:
    \begin{equation}
        \text{Logistic}(z) = \text{sigmoid}(z) = \sigma(z) = \frac{1}{1 + e^{-z}}
    \end{equation}
\end{definition}
We plot the logistic function in Figure \ref{fig:logistic-function}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/4_logistic-function.png}
    \caption{The Logistic Function}
    \label{fig:logistic-function}
\end{figure}
How can we implement this in our regression modeling?

\subsection{The Model}

Before, we had a model of the form:
\begin{equation}
    \widehat{y}_i = \beta_0 + \beta_1 x_i
\end{equation}
Now, we modify this model so that our predictions $\widehat{y}_i$ are between 0 and 1. We can do this by making our response variable $\widehat{y}_i$ the output of the logistic function:
\begin{equation}
    \widehat{y}_i = \text{Logistic}(\beta_0 + \beta_1 x_i) = \frac{1}{1 + e^{-\beta_0 + \beta_1 x_i}}
\end{equation}
This is a \textbf{logistic regression model}, and allows us to model the \textbf{probability} that a putt is sunk directly. Mathematically, we can write this model as:
\begin{align}
    p_i &= \P(y_i = 1 \mid x_i) = \frac{1}{1 + e^{-(x_i^T\boldsymbol{\beta})}} \\
    &\text{where } y_i \sim \text{Bernoulli}(p_i )= \begin{cases}
        1 & \text{w.p. } p_i \\
        0 & \text{w.p. } 1-p_i
    \end{cases}
\end{align}

\subsection{Estimating the Coefficients}

Our data is in terms of $y_i \in \{0, 1\}$ and $x_i \in \mathbb{R}$, not in terms of $p_i \in [0, 1]$. How can we then estimate the coefficients $\boldsymbol{\beta}$ in logistic regression?

In linear regression, we estimate $\boldsymbol{\beta}$ by minimizing the Residual Sum of Squares (RSS), or the squared error.
\begin{definition}[Residual Sum of Squares]
    The \textbf{Residual Sum of Squares} (RSS) is defined as:
    \begin{align}
        \text{RSS} &= \sum_{i=1}^n (y_i - \widehat{y}_i)^2 \nonumber \\
        &= \sum_{i=1}^n (y_i - x_i^T\boldsymbol{\beta})^2
    \end{align}
\end{definition}
In logistic regression, we estimate $\boldsymbol{\beta}$ by minimizing the \textbf{log loss}, or the cross-entropy loss.
\begin{definition}[Log Loss]
    The \textbf{log loss} is defined as:
    \begin{align}
        L(\boldsymbol{\beta}) &= -\frac{1}{n}\sum_{i=1}^n \left(y_i \log p_i + (1 - y_i) \log (1 - p_i)\right) \nonumber \\
        &\text{where } p_i = \mathbb{P}(y_i = 1 \mid x_i, \boldsymbol{\beta}) = \frac{1}{1 + e^{-(x_i^T\boldsymbol{\beta})}}
    \end{align}
\end{definition}
\begin{enumerate}
    \item If $y_i = 1$, then $L(\boldsymbol{\beta}) = -\log p_i$
    \begin{itemize}
        \item[-] If $p_1 \approx 1$, then $\log p_i$ is high and $L(\boldsymbol{\beta})$ is low
        \item[-] If $p_1 \approx 0$, then $\log p_i$ is low and $L(\boldsymbol{\beta})$ is high
    \end{itemize}
    \item If $y_i = 0$, then $L(\boldsymbol{\beta}) = -\log (1 - p_i)$
    \begin{itemize}
        \item[-] If $p_1 \approx 1$, then $\log (1 - p_i)$ is low and $L(\boldsymbol{\beta})$ is high
        \item[-] If $p_1 \approx 0$, then $\log (1 - p_i)$ is high and $L(\boldsymbol{\beta})$ is low
    \end{itemize}
\end{enumerate}
The log loss is a \textbf{convex} function, which means that it has a single global minimum. This means we can solve for $\boldsymbol{\beta}$ by setting its gradient equal to 0.
\begin{align}
    \nabla_{\boldsymbol{\beta}} L(\boldsymbol{\beta}) &= -\frac{1}{n}\sum_{i=1}^n \left(y_i \nabla_{\boldsymbol{\beta}} \log p_i + (1 - y_i) \nabla_{\boldsymbol{\beta}} \log (1 - p_i)\right) \nonumber \\
    &= 0
\end{align}
Solving for $\boldsymbol{\beta}$ analytically can be difficult, so we use gradient descent in \texttt{R} to find the minimum iteratively.

\section{Back to Sinking Putts}

Recall the variables we have:
\begin{itemize}
    \item[-] $i$: index of the $i^{th}$ putt in our dataset
    \item[-] $y_i$: 1 if the $i^{th}$ putt was sunk, 0 otherwise
    \item[-] $x_i$: distance to the hole of the $i^{th}$ putt
\end{itemize}

\subsection{Fitting the Model}

We use the \texttt{glm} function to fit the logistic regression model.
\begin{lstlisting}[language=R]
    model = glm(putt_made ~ distance, data = putt_data, family = "binomial")
    summary(model)
\end{lstlisting}
We plot the fitted line against the data in Figure \ref{fig:putt-logistic}. It seems to fit the data well, but we should check the model's performance from longer distances.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/4_putt-logistic.png}
    \caption{Putt data and fitted logistic regression}
    \label{fig:putt-logistic}
\end{figure}
In Figure \ref{fig:putt-logistic-zoomed}, we see that the model is able to extrapolate when $x_i > 20$ because we forced our output to be between 0 and 1.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/4_putt-logistic-zoomed.png}
    \caption{Putt data and fitted logistic regression (zoomed out)}
    \label{fig:putt-logistic-zoomed}
\end{figure}
\subsection{Improving the Model}

We can do even better by modeling the log odds with a cubic polynomial. Our new model is:
\begin{equation}
    \mathbb{P}(y_i = 1 \mid x_i) = \frac{1}{1 + e^{-\beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3}}
\end{equation}
We fit this model in \texttt{R} with the following code:
\begin{lstlisting}[language=R]
    model = glm(putt_made ~ distance + I(distance^2) + I(distance^3),
                data = putt_data, family = "binomial")
    summary(model)
\end{lstlisting}
We plot the fitted line against the data in Figure \ref{fig:putt-logistic-cubic}, and we can see that the model is able to extrapolate when $x_i > 20$.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/4_putt-logistic-cubic.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/4_putt-logistic-cubic-zoomed.png}
    \end{minipage}
    \caption{Putt data and fitted logistic regression with a cubic polynomial}
    \label{fig:putt-logistic-cubic}
\end{figure}

\subsection{Takeaways}

What can we take away from this example?
\begin{itemize}
    \item[-] Use linear regression to predict a value that can take any real number
    \item[-] Use logistic regression to predict a \textbf{probability} in $[0,1]$
\end{itemize}

\end{document}